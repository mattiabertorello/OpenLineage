plugins {
    id "java"
    id "java-library"
    id "java-test-fixtures"
    id "com.diffplug.spotless" version "6.12.0"
    id "pmd"
    id("io.openlineage.print-source-set-configuration")
}

pmd {
    consoleOutput = true
    toolVersion = "6.46.0"
    rulesMinimumPriority = 5
    ruleSetFiles = rootProject.files("pmd-openlineage.xml")
    ruleSets = []
    ignoreFailures = true
}

pmdMain {
    reports {
        html.required = true
    }
}

archivesBaseName = "openlineage-spark-shared"

repositories {
    mavenLocal()
    mavenCentral()
    maven {
        url = "https://astronomer.jfrog.io/artifactory/maven-public-libs-snapshot"
    }
}

ext {
    assertjVersion = "3.25.1"
    bigqueryVersion = "0.29.0"
    junit5Version = "5.10.1"
    sparkVersion = "3.2.2"
    sparkVersionShort = "3.2"
    scalaVersion = project.getProperty("scala.binary.version")
    snowflakeVersion = "2.11.0"
    postgresqlVersion = "42.7.1"
    mockitoVersion = "4.11.0"
    testcontainersVersion = "1.19.3"

    // What's going here?
    // This two properties control which versions of Spark and Scala are used to write the code.
    // This controls the packages that your IDE, like IntelliJ, will use to provide code completion.
    // Why do we need this? Because the API of Apache Spark evolves over time, and keeping the
    // versions unchanging will show us that sometimes the code we write isn't compatible with differing
    // versions of Spark.
    defaultSparkVersion = "3.2.4"
    defaultSparkSeries = defaultSparkVersion.substring(0, 3)
    defaultScalaVersion = "2.12"

    scala212SparkVersion = "3.2.4"
    scala212SparkSeries = scala212SparkVersion.substring(0, 3)

    scala213SparkVersion = "3.2.4"
    scala213SparkSeries = scala213SparkVersion.substring(0, 3)
}

configurations {
    // This makes this configuration inherit all dependencies from the 'api' configuration
    scala213Api.extendsFrom(api)
}

sourceSets {
    scala213 {
        compileClasspath += configurations.scala213Api
        runtimeClasspath += configurations.scala213Api

        java {
            srcDir("src/main/java")
        }
        resources {
            srcDir("src/main/resources")
        }
    }

    testScala213 {
        compileClasspath += sourceSets.scala213.output + sourceSets.scala213.compileClasspath
        runtimeClasspath += sourceSets.scala213.output + sourceSets.scala213.runtimeClasspath
        java {
            srcDir("src/test/java")
        }
        resources {
            srcDir("src/test/resources")
        }
    }
}

dependencies {
    api("io.openlineage:openlineage-java:${project.version}")
    api("io.openlineage:openlineage-sql-java:${project.version}")

    compileOnly("org.apache.spark:spark-core_${defaultScalaVersion}:${defaultSparkVersion}")
    compileOnly("org.apache.spark:spark-sql_${defaultScalaVersion}:${defaultSparkVersion}")
    compileOnly("com.google.cloud.spark:spark-bigquery-with-dependencies_${defaultScalaVersion}:${bigqueryVersion}") {
        exclude group: "com.fasterxml.jackson.core"
        exclude group: "com.fasterxml.jackson.module"
    }
    compileOnly("net.snowflake:spark-snowflake_${defaultScalaVersion}:${snowflakeVersion}-spark_${defaultSparkSeries}") {
        exclude group: "com.google.guava:guava"
        exclude group: "org.apache.spark:spark-core_${defaultScalaVersion}"
        exclude group: "org.apache.spark:spark-sql_${defaultScalaVersion}"
        exclude group: "org.apache.spark:spark-catalyst_${defaultScalaVersion}"
    }

    compileOnly("org.apache.spark:spark-hive_${defaultScalaVersion}:${defaultSparkVersion}")
    compileOnly("org.apache.spark:spark-sql-kafka-0-10_${defaultScalaVersion}:${defaultSparkVersion}")
    compileOnly("com.databricks:databricks-dbutils-scala_${defaultScalaVersion}:0.1.4") {
        exclude group: "com.fasterxml.jackson.core"
    }

    testImplementation(platform("org.junit:junit-bom:${junit5Version}"))
    testImplementation("org.junit.jupiter:junit-jupiter")
    testImplementation("org.junit.jupiter:junit-jupiter-params")

    testImplementation("org.postgresql:postgresql:${postgresqlVersion}")
    testImplementation("org.xerial:sqlite-jdbc:3.44.1.0")
    testImplementation("org.apache.kafka:kafka-clients:3.6.1")
    testImplementation("org.mock-server:mockserver-client-java:5.14.0") {
        exclude group: "com.google.guava", module: "guava"
        exclude group: "com.fasterxml.jackson.core"
        exclude group: "com.fasterxml.jackson.datatype"
        exclude group: "com.fasterxml.jackson.dataformat"
    }
    testImplementation("org.awaitility:awaitility:4.2.0")
    testImplementation("org.assertj:assertj-core:${assertjVersion}")
    testImplementation("org.mockito:mockito-core:${mockitoVersion}")
    testImplementation("org.mockito:mockito-inline:${mockitoVersion}")
    testImplementation("org.mockito:mockito-junit-jupiter:${mockitoVersion}")
    testImplementation("org.apache.spark:spark-core_${defaultScalaVersion}:${defaultSparkVersion}")
    testImplementation("org.apache.spark:spark-sql_${defaultScalaVersion}:${defaultSparkVersion}")
    testImplementation("org.apache.spark:spark-hive_${defaultScalaVersion}:${defaultSparkVersion}")
    testImplementation("com.google.cloud.spark:spark-bigquery-with-dependencies_${defaultScalaVersion}:${bigqueryVersion}")


    scala213CompileOnly("org.projectlombok:lombok:${lombokVersion}")
    scala213CompileOnly("org.apache.spark:spark-core_2.13:${scala213SparkVersion}")
    scala213CompileOnly("org.apache.spark:spark-sql_2.13:${scala213SparkVersion}")
    scala213CompileOnly("org.apache.spark:spark-hive_2.13:${scala213SparkVersion}")
    scala213CompileOnly("org.apache.spark:spark-sql-kafka-0-10_2.13:${scala213SparkVersion}")

    scala213CompileOnly("com.google.cloud.spark:spark-bigquery-with-dependencies_2.13:${bigqueryVersion}") {
        exclude group: "com.fasterxml.jackson.core"
        exclude group: "com.fasterxml.jackson.module"
    }

    scala213CompileOnly("net.snowflake:spark-snowflake_2.13:${snowflakeVersion}-spark_${scala213SparkSeries}") {
        exclude group: "com.google.guava:guava"
        exclude group: "org.apache.spark:spark-core_2.13"
        exclude group: "org.apache.spark:spark-sql_2.13"
        exclude group: "org.apache.spark:spark-catalyst_2.13"
    }

    scala213CompileOnly("com.databricks:databricks-dbutils-scala_2.13:0.1.4") {
        exclude group: "com.fasterxml.jackson.core"
    }

    testScala213Implementation("org.assertj:assertj-core:${assertjVersion}")
    testScala213Implementation("org.awaitility:awaitility:4.2.0")
    testScala213Implementation(platform("org.junit:junit-bom:${junit5Version}"))
    testScala213Implementation("org.junit.jupiter:junit-jupiter")
    testScala213Implementation("org.junit.jupiter:junit-jupiter-api")
    testScala213Implementation("org.mockito:mockito-core:${mockitoVersion}")
    testScala213Implementation("org.mockito:mockito-inline:${mockitoVersion}")
    testScala213Implementation("org.postgresql:postgresql:${postgresqlVersion}")
    testScala213Implementation("org.apache.spark:spark-core_2.13:${scala213SparkVersion}")
    testScala213Implementation("org.apache.spark:spark-sql_2.13:${scala213SparkVersion}")
    testScala213Implementation("org.apache.spark:spark-hive_2.13:${scala213SparkVersion}")
    testScala213Implementation("com.google.cloud.spark:spark-bigquery-with-dependencies_2.13:${bigqueryVersion}")
}

def commonTestConfiguration = { sparkVersion, scalaVersion ->
    forkEvery 1
    maxParallelForks 5
    testLogging {
        events "passed", "skipped", "failed"
        showStandardStreams = true
    }
    systemProperties = [
            "junit.platform.output.capture.stdout": "true",
            "junit.platform.output.capture.stderr": "true",
            "spark.version"                       : "${sparkVersion}",
            "openlineage.spark.jar"               : "${archivesBaseName}-${project.version}.jar",
            "kafka.package.version"               : "org.apache.spark:spark-sql-kafka-0-10_${scalaVersion}:${sparkVersion}",
            "mockserver.logLevel"                 : "ERROR"
    ]
}

tasks.withType(Test).configureEach {
    useJUnitPlatform()

    forkEvery(1)
    maxParallelForks(5)

    testLogging {
        events "passed", "skipped", "failed"
        showStandardStreams = true
    }
}

tasks.register("testScala213", Test) {
    dependsOn(tasks.named("compileScala213Java"))
    description = "Runs all tests in this module using Apache Spark 3.3.2 libraries compiled with Scala 2.13"
    group = "verification"
    testClassesDirs = sourceSets.testScala213.output.classesDirs
    classpath = sourceSets.testScala213.runtimeClasspath
}

tasks.register("testAll") {
    dependsOn(tasks.named("test"), tasks.named("testScala213"))
}

spotless {
    def disallowWildcardImports = {
        String text = it
        def regex = ~/import .*\.\*;/
        def m = regex.matcher(text)
        if (m.find()) {
            throw new AssertionError("Wildcard imports disallowed - ${m.findAll()}")
        }
    }
    java {
        googleJavaFormat()
        removeUnusedImports()
        custom "disallowWildcardImports", disallowWildcardImports
    }
}

tasks.withType(JavaCompile).configureEach {
    options.annotationProcessorPath = configurations.annotationProcessor
}

tasks.build.dependsOn(tasks.named("testAll"))
